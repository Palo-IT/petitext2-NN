{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R-NET AND THE QUESTION CATEGORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Concatenate the python files in the notebook</b>\n",
    "\n",
    "<B>The question category has been added</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepro.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import ujson as json\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "\n",
    "def convert_idx(text, tokens):\n",
    "    current = 0\n",
    "    spans = []\n",
    "    for token in tokens:\n",
    "        current = text.find(token, current)\n",
    "        if current < 0:\n",
    "            print(\"Token {} cannot be found\".format(token))\n",
    "            raise Exception()\n",
    "        spans.append((current, current + len(token)))\n",
    "        current += len(token)\n",
    "    return spans\n",
    "\n",
    "##########################################################\n",
    "# 09/05/2018\n",
    "# add the question category as an argument in the method\n",
    "##########################################################\n",
    "# def process_file(filename, data_type, word_counter, char_counter):\n",
    "def process_file(filename, data_type, word_counter, char_counter, ques_cat_filename):\n",
    "    print(\"Generating {} examples...\".format(data_type))\n",
    "    examples = []\n",
    "    eval_examples = {}\n",
    "    total = 0\n",
    "    \n",
    "    ##############################################\n",
    "    # 09/05/2018\n",
    "    # get the questions gategories from the file \n",
    "    ##############################################\n",
    "    categories_questions_list = []\n",
    "\n",
    "    with open(ques_cat_filename, 'r') as cat_ques_dev:\n",
    "\n",
    "        categories_questions_list = [line.split(',') for line in cat_ques_dev.readlines()]\n",
    "\n",
    "        categories_questions = categories_questions_list[0]\n",
    "        \n",
    "    with open(filename, \"r\") as fh:\n",
    "        source = json.load(fh)\n",
    "        for article in tqdm(source[\"data\"]):\n",
    "            for para in article[\"paragraphs\"]:\n",
    "                context = para[\"context\"].replace(\n",
    "                    \"''\", '\" ').replace(\"``\", '\" ')\n",
    "                context_tokens = word_tokenize(context)\n",
    "                context_chars = [list(token) for token in context_tokens]\n",
    "                spans = convert_idx(context, context_tokens)\n",
    "                for token in context_tokens:\n",
    "                    word_counter[token] += len(para[\"qas\"])\n",
    "                    for char in token:\n",
    "                        char_counter[char] += len(para[\"qas\"])\n",
    "                for qa in para[\"qas\"]:\n",
    "                    \n",
    "                    #########################################################\n",
    "                    # 30/04/2018\n",
    "                    # Get the category of the question\n",
    "                    # and add the question's category token in the word_counter\n",
    "                    # also the char of the category\n",
    "                    ###########################################################\n",
    "                    ques_category_token = categories_questions[total]\n",
    "                    word_counter[ques_category_token] += 1\n",
    "                    \n",
    "                    ##for the moment, only the question category will be added\n",
    "                    # ques_category_chars = list(ques_category_token)\n",
    "                    # for char in ques_category_chars:\n",
    "                    #    char_counter[char] += 1\n",
    "                    \n",
    "                    total += 1\n",
    "                    ques = qa[\"question\"].replace(\n",
    "                        \"''\", '\" ').replace(\"``\", '\" ')\n",
    "                    ques_tokens = word_tokenize(ques)\n",
    "                    ques_chars = [list(token) for token in ques_tokens]\n",
    "                    for token in ques_tokens:\n",
    "                        word_counter[token] += 1\n",
    "                        for char in token:\n",
    "                            char_counter[char] += 1\n",
    "                    y1s, y2s = [], []\n",
    "                    answer_texts = []\n",
    "                    for answer in qa[\"answers\"]:\n",
    "                        answer_text = answer[\"text\"]\n",
    "                        answer_start = answer['answer_start']\n",
    "                        answer_end = answer_start + len(answer_text)\n",
    "                        answer_texts.append(answer_text)\n",
    "                        answer_span = []\n",
    "                        for idx, span in enumerate(spans):\n",
    "                            if not (answer_end <= span[0] or answer_start >= span[1]):\n",
    "                                answer_span.append(idx)\n",
    "                        y1, y2 = answer_span[0], answer_span[-1]\n",
    "                        y1s.append(y1)\n",
    "                        y2s.append(y2)\n",
    "                        \n",
    "                    #####################################################\n",
    "                    # 27/04/2018\n",
    "                    # Add the question category in the example dictionary\n",
    "                    #####################################################\n",
    "#                     example = {\"context_tokens\": context_tokens, \"context_chars\": context_chars, \n",
    "#                                \"ques_tokens\": ques_tokens, \"ques_chars\": ques_chars, \n",
    "#                               \"y1s\": y1s, \"y2s\": y2s, \"id\": total}\n",
    "\n",
    "                    example = {\"context_tokens\": context_tokens, \"context_chars\": context_chars, \n",
    "                               \"ques_tokens\": ques_tokens, \"ques_chars\": ques_chars, \n",
    "                               \"y1s\": y1s, \"y2s\": y2s, \"id\": total,\n",
    "                               \"ques_category_token\" : ques_category_token } #,\n",
    "    \n",
    "                               # for the moment, only the token of the question category is been added\n",
    "                               #\"ques_category_chars\" : ques_category_chars,}\n",
    "    \n",
    "                    examples.append(example)\n",
    "                    eval_examples[str(total)] = {\n",
    "                        \"context\": context, \"spans\": spans, \"answers\": answer_texts, \"uuid\": qa[\"id\"]}\n",
    "        random.shuffle(examples)\n",
    "        print(\"{} questions in total\".format(len(examples)))\n",
    "    return examples, eval_examples\n",
    "\n",
    "\n",
    "def get_embedding(counter, data_type, limit=-1, emb_file=None, size=None, vec_size=None, token2idx_dict=None):\n",
    "    print(\"Generating {} embedding...\".format(data_type))\n",
    "    embedding_dict = {}\n",
    "    filtered_elements = [k for k, v in counter.items() if v > limit]\n",
    "    if emb_file is not None:\n",
    "        assert size is not None\n",
    "        assert vec_size is not None\n",
    "        with open(emb_file, \"r\", encoding=\"utf-8\") as fh:\n",
    "            for line in tqdm(fh, total=size):\n",
    "                array = line.split()\n",
    "                word = \"\".join(array[0:-vec_size])\n",
    "                vector = list(map(float, array[-vec_size:]))\n",
    "                if word in counter and counter[word] > limit:\n",
    "                    embedding_dict[word] = vector\n",
    "        print(\"{} / {} tokens have corresponding {} embedding vector\".format(\n",
    "            len(embedding_dict), len(filtered_elements), data_type))\n",
    "    else:\n",
    "        assert vec_size is not None\n",
    "        for token in filtered_elements:\n",
    "            embedding_dict[token] = [np.random.normal(\n",
    "                scale=0.01) for _ in range(vec_size)]\n",
    "        print(\"{} tokens have corresponding embedding vector\".format(\n",
    "            len(filtered_elements)))\n",
    "\n",
    "    NULL = \"--NULL--\"\n",
    "    OOV = \"--OOV--\"\n",
    "    token2idx_dict = {token: idx for idx, token in enumerate(\n",
    "        embedding_dict.keys(), 2)} if token2idx_dict is None else token2idx_dict\n",
    "    token2idx_dict[NULL] = 0\n",
    "    token2idx_dict[OOV] = 1\n",
    "    embedding_dict[NULL] = [0. for _ in range(vec_size)]\n",
    "    embedding_dict[OOV] = [0. for _ in range(vec_size)]\n",
    "    idx2emb_dict = {idx: embedding_dict[token]\n",
    "                    for token, idx in token2idx_dict.items()}\n",
    "    emb_mat = [idx2emb_dict[idx] for idx in range(len(idx2emb_dict))]\n",
    "    return emb_mat, token2idx_dict\n",
    "\n",
    "\n",
    "def build_features(config, examples, data_type, out_file, word2idx_dict, char2idx_dict, is_test=False):\n",
    "\n",
    "    para_limit = config.test_para_limit if is_test else config.para_limit\n",
    "    ques_limit = config.test_ques_limit if is_test else config.ques_limit\n",
    "    char_limit = config.char_limit\n",
    "\n",
    "    def filter_func(example, is_test=False):\n",
    "        return len(example[\"context_tokens\"]) > para_limit or len(example[\"ques_tokens\"]) > ques_limit\n",
    "\n",
    "    print(\"Processing {} examples...\".format(data_type))\n",
    "    writer = tf.python_io.TFRecordWriter(out_file)\n",
    "    total = 0\n",
    "    total_ = 0\n",
    "    meta = {}\n",
    "    for example in tqdm(examples):\n",
    "        total_ += 1\n",
    "\n",
    "        if filter_func(example, is_test):\n",
    "            continue\n",
    "\n",
    "        total += 1\n",
    "        context_idxs = np.zeros([para_limit], dtype=np.int32)\n",
    "        context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32)\n",
    "        ques_idxs = np.zeros([ques_limit], dtype=np.int32)\n",
    "        ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32)\n",
    "        y1 = np.zeros([para_limit], dtype=np.float32)\n",
    "        y2 = np.zeros([para_limit], dtype=np.float32)\n",
    "        \n",
    "        ########################################################\n",
    "        # 09/05/2018\n",
    "        # create a question category idxs with ques_category_limit length\n",
    "        ########################################################\n",
    "        ques_category_idxs = np.zeros([ques_category_limit], dtype=np.int32)\n",
    "        #ques_category_char_idxs = np.zeros([ques_category_limit, ques_category_char_limit], dtype=np.int32)\n",
    "        \n",
    "        def _get_word(word):\n",
    "            for each in (word, word.lower(), word.capitalize(), word.upper()):\n",
    "                if each in word2idx_dict:\n",
    "                    return word2idx_dict[each]\n",
    "            return 1\n",
    "\n",
    "        def _get_char(char):\n",
    "            if char in char2idx_dict:\n",
    "                return char2idx_dict[char]\n",
    "            return 1\n",
    "\n",
    "        for i, token in enumerate(example[\"context_tokens\"]):\n",
    "            context_idxs[i] = _get_word(token)\n",
    "\n",
    "        for i, token in enumerate(example[\"ques_tokens\"]):\n",
    "            ques_idxs[i] = _get_word(token)\n",
    "\n",
    "        for i, token in enumerate(example[\"context_chars\"]):\n",
    "            for j, char in enumerate(token):\n",
    "                if j == char_limit:\n",
    "                    break\n",
    "                context_char_idxs[i, j] = _get_char(char)\n",
    "\n",
    "        for i, token in enumerate(example[\"ques_chars\"]):\n",
    "            for j, char in enumerate(token):\n",
    "                if j == char_limit:\n",
    "                    break\n",
    "                ques_char_idxs[i, j] = _get_char(char)\n",
    "                \n",
    "        ################################################################\n",
    "        # 7/05/2018\n",
    "        # get the word from \"ques_category\"\n",
    "        # and put in the ques_category_idxs\n",
    "        ################################################################\n",
    "        ques_category_token = example[\"ques_category_token\"]\n",
    "        ques_category_word = _get_word(ques_category_token)\n",
    "        ques_category_idxs[0] = ques_category_word\n",
    "        \n",
    "#         for j, char in enumerate(example[\"ques_category_chars\"]):\n",
    "#             #print('j : {} - char : {}'.format(j,char))\n",
    "#             if j == ques_category_char_limit:\n",
    "#                 break\n",
    "#             ques_category_char_idxs[0, j] = _get_char(char) \n",
    "\n",
    "        start, end = example[\"y1s\"][-1], example[\"y2s\"][-1]\n",
    "        y1[start], y2[end] = 1.0, 1.0\n",
    "\n",
    "        ################################################################\n",
    "        # 09/05/2018\n",
    "        # register the question category in the record\n",
    "        ################################################################\n",
    "#         record = tf.train.Example(features=tf.train.Features(feature={\n",
    "#                                   \"context_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_idxs.tostring()])),\n",
    "#                                   \"ques_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_idxs.tostring()])),\n",
    "#                                   \"context_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_char_idxs.tostring()])),\n",
    "#                                   \"ques_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_char_idxs.tostring()])),\n",
    "#                                   \"y1\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[y1.tostring()])),\n",
    "#                                   \"y2\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[y2.tostring()])),\n",
    "#                                   \"id\": tf.train.Feature(int64_list=tf.train.Int64List(value=[example[\"id\"]]))\n",
    "#                                   }))\n",
    "        \n",
    "        record = tf.train.Example(features=tf.train.Features(feature={\n",
    "                                  \"context_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_idxs.tostring()])),\n",
    "                                  \"ques_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_idxs.tostring()])),\n",
    "                                  \"context_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_char_idxs.tostring()])),\n",
    "                                  \"ques_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_char_idxs.tostring()])),\n",
    "                                  \"y1\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[y1.tostring()])),\n",
    "                                  \"y2\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[y2.tostring()])),\n",
    "                                  \"id\": tf.train.Feature(int64_list=tf.train.Int64List(value=[example[\"id\"]])),\n",
    "                                  \"ques_category_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_category_idxs.tostring()])) #,\n",
    "                                  # for the moment the character is not added\n",
    "                                  # \"ques_category_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_category_char_idxs.tostring()]))  \n",
    "                                  }))\n",
    "        writer.write(record.SerializeToString())\n",
    "    print(\"Build {} / {} instances of features in total\".format(total, total_))\n",
    "    meta[\"total\"] = total\n",
    "    writer.close()\n",
    "    return meta\n",
    "\n",
    "\n",
    "def save(filename, obj, message=None):\n",
    "    if message is not None:\n",
    "        print(\"Saving {}...\".format(message))\n",
    "        with open(filename, \"w\") as fh:\n",
    "            json.dump(obj, fh)\n",
    "\n",
    "\n",
    "def prepro(config):\n",
    "    word_counter, char_counter = Counter(), Counter()\n",
    "    \n",
    "    ##########################################################################\n",
    "    # 09/05/2018\n",
    "    # add the question category file as argument in the process_file function\n",
    "    ##########################################################################\n",
    "#     train_examples, train_eval = process_file(\n",
    "#         config.train_file, \"train\", word_counter, char_counter)\n",
    "#     dev_examples, dev_eval = process_file(\n",
    "#         config.dev_file, \"dev\", word_counter, char_counter)\n",
    "#     test_examples, test_eval = process_file(\n",
    "#         config.test_file, \"test\", word_counter, char_counter)\n",
    "    \n",
    "    train_examples, train_eval = process_file(\n",
    "        config.train_file, \"train\", word_counter, char_counter, config.train_ques_categories_file)\n",
    "    dev_examples, dev_eval = process_file(\n",
    "        config.dev_file, \"dev\", word_counter, char_counter, config.dev_ques_categories_file)\n",
    "    test_examples, test_eval = process_file(\n",
    "        config.test_file, \"test\", word_counter, char_counter, config.test_ques_categories_file)\n",
    "\n",
    "    word_emb_file = config.fasttext_file if config.fasttext else config.glove_word_file\n",
    "    char_emb_file = config.glove_char_file if config.pretrained_char else None\n",
    "    char_emb_size = config.glove_char_size if config.pretrained_char else None\n",
    "    char_emb_dim = config.glove_dim if config.pretrained_char else config.char_dim\n",
    "\n",
    "    word2idx_dict = None\n",
    "    if os.path.isfile(config.word2idx_file):\n",
    "        with open(config.word2idx_file, \"r\") as fh:\n",
    "            word2idx_dict = json.load(fh)\n",
    "    word_emb_mat, word2idx_dict = get_embedding(word_counter, \"word\", emb_file=word_emb_file,\n",
    "                                                size=config.glove_word_size, vec_size=config.glove_dim, token2idx_dict=word2idx_dict)\n",
    "\n",
    "    char2idx_dict = None\n",
    "    if os.path.isfile(config.char2idx_file):\n",
    "        with open(config.char2idx_file, \"r\") as fh:\n",
    "            char2idx_dict = json.load(fh)\n",
    "    char_emb_mat, char2idx_dict = get_embedding(\n",
    "        char_counter, \"char\", emb_file=char_emb_file, size=char_emb_size, vec_size=char_emb_dim, token2idx_dict=char2idx_dict)\n",
    "\n",
    "    build_features(config, train_examples, \"train\",\n",
    "                   config.train_record_file, word2idx_dict, char2idx_dict)\n",
    "    dev_meta = build_features(config, dev_examples, \"dev\",\n",
    "                              config.dev_record_file, word2idx_dict, char2idx_dict)\n",
    "    test_meta = build_features(config, test_examples, \"test\",\n",
    "                               config.test_record_file, word2idx_dict, char2idx_dict, is_test=True)\n",
    "\n",
    "    save(config.word_emb_file, word_emb_mat, message=\"word embedding\")\n",
    "    save(config.char_emb_file, char_emb_mat, message=\"char embedding\")\n",
    "    save(config.train_eval_file, train_eval, message=\"train eval\")\n",
    "    save(config.dev_eval_file, dev_eval, message=\"dev eval\")\n",
    "    save(config.test_eval_file, test_eval, message=\"test eval\")\n",
    "    save(config.dev_meta, dev_meta, message=\"dev meta\")\n",
    "    save(config.word2idx_file, word2idx_dict, message=\"word2idx\")\n",
    "    save(config.char2idx_file, char2idx_dict, message=\"char2idx\")\n",
    "    save(config.test_meta, test_meta, message=\"test meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#func.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "INF = 1e30\n",
    "\n",
    "\n",
    "class cudnn_gru:\n",
    "\n",
    "    def __init__(self, num_layers, num_units, batch_size, input_size, keep_prob=1.0, is_train=None, scope=None):\n",
    "        self.num_layers = num_layers\n",
    "        self.grus = []\n",
    "        self.inits = []\n",
    "        self.dropout_mask = []\n",
    "        for layer in range(num_layers):\n",
    "            input_size_ = input_size if layer == 0 else 2 * num_units\n",
    "            gru_fw = tf.contrib.cudnn_rnn.CudnnGRU(1, num_units)\n",
    "            gru_bw = tf.contrib.cudnn_rnn.CudnnGRU(1, num_units)\n",
    "            init_fw = tf.tile(tf.Variable(\n",
    "                tf.zeros([1, 1, num_units])), [1, batch_size, 1])\n",
    "            init_bw = tf.tile(tf.Variable(\n",
    "                tf.zeros([1, 1, num_units])), [1, batch_size, 1])\n",
    "            mask_fw = dropout(tf.ones([1, batch_size, input_size_], dtype=tf.float32),\n",
    "                              keep_prob=keep_prob, is_train=is_train, mode=None)\n",
    "            mask_bw = dropout(tf.ones([1, batch_size, input_size_], dtype=tf.float32),\n",
    "                              keep_prob=keep_prob, is_train=is_train, mode=None)\n",
    "            self.grus.append((gru_fw, gru_bw, ))\n",
    "            self.inits.append((init_fw, init_bw, ))\n",
    "            self.dropout_mask.append((mask_fw, mask_bw, ))\n",
    "\n",
    "    def __call__(self, inputs, seq_len, keep_prob=1.0, is_train=None, concat_layers=True):\n",
    "        outputs = [tf.transpose(inputs, [1, 0, 2])]\n",
    "        for layer in range(self.num_layers):\n",
    "            gru_fw, gru_bw = self.grus[layer]\n",
    "            init_fw, init_bw = self.inits[layer]\n",
    "            mask_fw, mask_bw = self.dropout_mask[layer]\n",
    "            with tf.variable_scope(\"fw_{}\".format(layer)):\n",
    "                out_fw, _ = gru_fw(\n",
    "                    outputs[-1] * mask_fw, initial_state=(init_fw, ))\n",
    "            with tf.variable_scope(\"bw_{}\".format(layer)):\n",
    "                inputs_bw = tf.reverse_sequence(\n",
    "                    outputs[-1] * mask_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)\n",
    "                out_bw, _ = gru_bw(inputs_bw, initial_state=(init_bw, ))\n",
    "                out_bw = tf.reverse_sequence(\n",
    "                    out_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)\n",
    "            outputs.append(tf.concat([out_fw, out_bw], axis=2))\n",
    "        if concat_layers:\n",
    "            res = tf.concat(outputs[1:], axis=2)\n",
    "        else:\n",
    "            res = outputs[-1]\n",
    "        res = tf.transpose(res, [1, 0, 2])\n",
    "        return res\n",
    "\n",
    "\n",
    "class native_gru:\n",
    "\n",
    "    def __init__(self, num_layers, num_units, batch_size, input_size, keep_prob=1.0, is_train=None, scope=\"native_gru\"):\n",
    "        self.num_layers = num_layers\n",
    "        self.grus = []\n",
    "        self.inits = []\n",
    "        self.dropout_mask = []\n",
    "        self.scope = scope\n",
    "        for layer in range(num_layers):\n",
    "            input_size_ = input_size if layer == 0 else 2 * num_units\n",
    "            gru_fw = tf.contrib.rnn.GRUCell(num_units)\n",
    "            gru_bw = tf.contrib.rnn.GRUCell(num_units)\n",
    "            init_fw = tf.tile(tf.Variable(\n",
    "                tf.zeros([1, num_units])), [batch_size, 1])\n",
    "            init_bw = tf.tile(tf.Variable(\n",
    "                tf.zeros([1, num_units])), [batch_size, 1])\n",
    "            mask_fw = dropout(tf.ones([batch_size, 1, input_size_], dtype=tf.float32),\n",
    "                              keep_prob=keep_prob, is_train=is_train, mode=None)\n",
    "            mask_bw = dropout(tf.ones([batch_size, 1, input_size_], dtype=tf.float32),\n",
    "                              keep_prob=keep_prob, is_train=is_train, mode=None)\n",
    "            self.grus.append((gru_fw, gru_bw, ))\n",
    "            self.inits.append((init_fw, init_bw, ))\n",
    "            self.dropout_mask.append((mask_fw, mask_bw, ))\n",
    "\n",
    "    def __call__(self, inputs, seq_len, keep_prob=1.0, is_train=None, concat_layers=True):\n",
    "        outputs = [inputs]\n",
    "        with tf.variable_scope(self.scope):\n",
    "            for layer in range(self.num_layers):\n",
    "                gru_fw, gru_bw = self.grus[layer]\n",
    "                init_fw, init_bw = self.inits[layer]\n",
    "                mask_fw, mask_bw = self.dropout_mask[layer]\n",
    "                with tf.variable_scope(\"fw_{}\".format(layer)):\n",
    "                    out_fw, _ = tf.nn.dynamic_rnn(\n",
    "                        gru_fw, outputs[-1] * mask_fw, seq_len, initial_state=init_fw, dtype=tf.float32)\n",
    "                with tf.variable_scope(\"bw_{}\".format(layer)):\n",
    "                    inputs_bw = tf.reverse_sequence(\n",
    "                        outputs[-1] * mask_bw, seq_lengths=seq_len, seq_dim=1, batch_dim=0)\n",
    "                    out_bw, _ = tf.nn.dynamic_rnn(\n",
    "                        gru_bw, inputs_bw, seq_len, initial_state=init_bw, dtype=tf.float32)\n",
    "                    out_bw = tf.reverse_sequence(\n",
    "                        out_bw, seq_lengths=seq_len, seq_dim=1, batch_dim=0)\n",
    "                outputs.append(tf.concat([out_fw, out_bw], axis=2))\n",
    "        if concat_layers:\n",
    "            res = tf.concat(outputs[1:], axis=2)\n",
    "        else:\n",
    "            res = outputs[-1]\n",
    "        return res\n",
    "\n",
    "\n",
    "class ptr_net:\n",
    "    def __init__(self, batch, hidden, keep_prob=1.0, is_train=None, scope=\"ptr_net\"):\n",
    "        self.gru = tf.contrib.rnn.GRUCell(hidden)\n",
    "        self.batch = batch\n",
    "        self.scope = scope\n",
    "        self.keep_prob = keep_prob\n",
    "        self.is_train = is_train\n",
    "        self.dropout_mask = dropout(tf.ones(\n",
    "            [batch, hidden], dtype=tf.float32), keep_prob=keep_prob, is_train=is_train)\n",
    "\n",
    "    def __call__(self, init, match, d, mask):\n",
    "        with tf.variable_scope(self.scope):\n",
    "            d_match = dropout(match, keep_prob=self.keep_prob,\n",
    "                              is_train=self.is_train)\n",
    "            inp, logits1 = pointer(d_match, init * self.dropout_mask, d, mask)\n",
    "            d_inp = dropout(inp, keep_prob=self.keep_prob,\n",
    "                            is_train=self.is_train)\n",
    "            _, state = self.gru(d_inp, init)\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            _, logits2 = pointer(d_match, state * self.dropout_mask, d, mask)\n",
    "            return logits1, logits2\n",
    "\n",
    "\n",
    "def dropout(args, keep_prob, is_train, mode=\"recurrent\"):\n",
    "    if keep_prob < 1.0:\n",
    "        noise_shape = None\n",
    "        scale = 1.0\n",
    "        shape = tf.shape(args)\n",
    "        if mode == \"embedding\":\n",
    "            noise_shape = [shape[0], 1]\n",
    "            scale = keep_prob\n",
    "        if mode == \"recurrent\" and len(args.get_shape().as_list()) == 3:\n",
    "            noise_shape = [shape[0], 1, shape[-1]]\n",
    "        args = tf.cond(is_train, lambda: tf.nn.dropout(\n",
    "            args, keep_prob, noise_shape=noise_shape) * scale, lambda: args)\n",
    "    return args\n",
    "\n",
    "\n",
    "def softmax_mask(val, mask):\n",
    "    return -INF * (1 - tf.cast(mask, tf.float32)) + val\n",
    "\n",
    "\n",
    "def pointer(inputs, state, hidden, mask, scope=\"pointer\"):\n",
    "    with tf.variable_scope(scope):\n",
    "        u = tf.concat([tf.tile(tf.expand_dims(state, axis=1), [\n",
    "            1, tf.shape(inputs)[1], 1]), inputs], axis=2)\n",
    "        s0 = tf.nn.tanh(dense(u, hidden, use_bias=False, scope=\"s0\"))\n",
    "        s = dense(s0, 1, use_bias=False, scope=\"s\")\n",
    "        s1 = softmax_mask(tf.squeeze(s, [2]), mask)\n",
    "        a = tf.expand_dims(tf.nn.softmax(s1), axis=2)\n",
    "        res = tf.reduce_sum(a * inputs, axis=1)\n",
    "        return res, s1\n",
    "\n",
    "\n",
    "def summ(memory, hidden, mask, keep_prob=1.0, is_train=None, scope=\"summ\"):\n",
    "    with tf.variable_scope(scope):\n",
    "        d_memory = dropout(memory, keep_prob=keep_prob, is_train=is_train)\n",
    "        s0 = tf.nn.tanh(dense(d_memory, hidden, scope=\"s0\"))\n",
    "        s = dense(s0, 1, use_bias=False, scope=\"s\")\n",
    "        s1 = softmax_mask(tf.squeeze(s, [2]), mask)\n",
    "        a = tf.expand_dims(tf.nn.softmax(s1), axis=2)\n",
    "        res = tf.reduce_sum(a * memory, axis=1)\n",
    "        return res\n",
    "\n",
    "\n",
    "def dot_attention(inputs, memory, mask, hidden, keep_prob=1.0, is_train=None, scope=\"dot_attention\"):\n",
    "    with tf.variable_scope(scope):\n",
    "\n",
    "        d_inputs = dropout(inputs, keep_prob=keep_prob, is_train=is_train)\n",
    "        d_memory = dropout(memory, keep_prob=keep_prob, is_train=is_train)\n",
    "        JX = tf.shape(inputs)[1]\n",
    "\n",
    "        with tf.variable_scope(\"attention\"):\n",
    "            inputs_ = tf.nn.relu(\n",
    "                dense(d_inputs, hidden, use_bias=False, scope=\"inputs\"))\n",
    "            memory_ = tf.nn.relu(\n",
    "                dense(d_memory, hidden, use_bias=False, scope=\"memory\"))\n",
    "            outputs = tf.matmul(inputs_, tf.transpose(\n",
    "                memory_, [0, 2, 1])) / (hidden ** 0.5)\n",
    "            mask = tf.tile(tf.expand_dims(mask, axis=1), [1, JX, 1])\n",
    "            logits = tf.nn.softmax(softmax_mask(outputs, mask))\n",
    "            outputs = tf.matmul(logits, memory)\n",
    "            res = tf.concat([inputs, outputs], axis=2)\n",
    "\n",
    "        with tf.variable_scope(\"gate\"):\n",
    "            dim = res.get_shape().as_list()[-1]\n",
    "            d_res = dropout(res, keep_prob=keep_prob, is_train=is_train)\n",
    "            gate = tf.nn.sigmoid(dense(d_res, dim, use_bias=False))\n",
    "            return res * gate\n",
    "\n",
    "\n",
    "def dense(inputs, hidden, use_bias=True, scope=\"dense\"):\n",
    "    with tf.variable_scope(scope):\n",
    "        shape = tf.shape(inputs)\n",
    "        dim = inputs.get_shape().as_list()[-1]\n",
    "        out_shape = [shape[idx] for idx in range(\n",
    "            len(inputs.get_shape().as_list()) - 1)] + [hidden]\n",
    "        flat_inputs = tf.reshape(inputs, [-1, dim])\n",
    "        W = tf.get_variable(\"W\", [dim, hidden])\n",
    "        res = tf.matmul(flat_inputs, W)\n",
    "        if use_bias:\n",
    "            b = tf.get_variable(\n",
    "                \"b\", [hidden], initializer=tf.constant_initializer(0.))\n",
    "            res = tf.nn.bias_add(res, b)\n",
    "        res = tf.reshape(res, out_shape)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#util.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "\n",
    "def get_record_parser(config, is_test=False):\n",
    "    def parse(example):\n",
    "        para_limit = config.test_para_limit if is_test else config.para_limit\n",
    "        ques_limit = config.test_ques_limit if is_test else config.ques_limit\n",
    "        char_limit = config.char_limit\n",
    "        \n",
    "        #######################################################################\n",
    "        # 09/05/2018\n",
    "        # because of the question category\n",
    "        ##################################################################\n",
    "#         features = tf.parse_single_example(example,\n",
    "#                                            features={\n",
    "#                                                \"context_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "#                                                \"ques_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "#                                                \"context_char_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "#                                                \"ques_char_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "#                                                \"y1\": tf.FixedLenFeature([], tf.string),\n",
    "#                                                \"y2\": tf.FixedLenFeature([], tf.string),\n",
    "#                                                \"id\": tf.FixedLenFeature([], tf.int64)\n",
    "#                                            })\n",
    "        \n",
    "        # add the question category idxs\n",
    "        features = tf.parse_single_example(example,\n",
    "                                           features={\n",
    "                                               \"context_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"ques_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"context_char_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"ques_char_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"y1\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"y2\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"id\": tf.FixedLenFeature([], tf.int64),\n",
    "                                               \"ques_category_idxs\": tf.FixedLenFeature([], tf.string) #,\n",
    "                                               # for the moment, only the token of the question category is added\n",
    "                                               #\"ques_category_char_idxs\": tf.FixedLenFeature([], tf.string)\n",
    "                                           })\n",
    "        \n",
    "        context_idxs = tf.reshape(tf.decode_raw(\n",
    "            features[\"context_idxs\"], tf.int32), [para_limit])\n",
    "        ques_idxs = tf.reshape(tf.decode_raw(\n",
    "            features[\"ques_idxs\"], tf.int32), [ques_limit])\n",
    "        context_char_idxs = tf.reshape(tf.decode_raw(\n",
    "            features[\"context_char_idxs\"], tf.int32), [para_limit, char_limit])\n",
    "        ques_char_idxs = tf.reshape(tf.decode_raw(\n",
    "            features[\"ques_char_idxs\"], tf.int32), [ques_limit, char_limit])\n",
    "        y1 = tf.reshape(tf.decode_raw(\n",
    "            features[\"y1\"], tf.float32), [para_limit])\n",
    "        y2 = tf.reshape(tf.decode_raw(\n",
    "            features[\"y2\"], tf.float32), [para_limit])\n",
    "        qa_id = features[\"id\"]\n",
    "        \n",
    "        #######################################################################\n",
    "        # 09/05/2018\n",
    "        # the question category idxs\n",
    "        ##################################################################\n",
    "        ques_category_idxs = tf.reshape(tf.decode_raw(\n",
    "            features[\"ques_category_idxs\"], tf.int32), [ques_category_limit])\n",
    "#         ques_category_char_idxs = tf.reshape(tf.decode_raw(\n",
    "#             features[\"ques_category_char_idxs\"], tf.int32), [ques_category_limit, ques_category_char_limit])\n",
    "        \n",
    "        #######################################################################\n",
    "        # 09/05/2018\n",
    "        # the question category idxs is added in the function return \n",
    "        ##################################################################\n",
    "        # return context_idxs, ques_idxs, context_char_idxs, ques_char_idxs, y1, y2, qa_id\n",
    "        return context_idxs, ques_idxs, context_char_idxs, ques_char_idxs, y1, y2, qa_id, ques_category_idxs #, ques_category_char_idxs\n",
    "    return parse\n",
    "\n",
    "\n",
    "def get_batch_dataset(record_file, parser, config):\n",
    "    num_threads = tf.constant(config.num_threads, dtype=tf.int32)\n",
    "    dataset = tf.data.TFRecordDataset(record_file).map(\n",
    "        parser, num_parallel_calls=num_threads).shuffle(config.capacity).repeat()\n",
    "    if config.is_bucket:\n",
    "        buckets = [tf.constant(num) for num in range(*config.bucket_range)]\n",
    "\n",
    "        def key_func(context_idxs, ques_idxs, context_char_idxs, ques_char_idxs, y1, y2, qa_id):\n",
    "            c_len = tf.reduce_sum(\n",
    "                tf.cast(tf.cast(context_idxs, tf.bool), tf.int32))\n",
    "            buckets_min = [np.iinfo(np.int32).min] + buckets\n",
    "            buckets_max = buckets + [np.iinfo(np.int32).max]\n",
    "            conditions_c = tf.logical_and(\n",
    "                tf.less(buckets_min, c_len), tf.less_equal(c_len, buckets_max))\n",
    "            bucket_id = tf.reduce_min(tf.where(conditions_c))\n",
    "            return bucket_id\n",
    "\n",
    "        def reduce_func(key, elements):\n",
    "            return elements.batch(config.batch_size)\n",
    "\n",
    "        dataset = dataset.apply(tf.contrib.data.group_by_window(\n",
    "            key_func, reduce_func, window_size=5 * config.batch_size)).shuffle(len(buckets) * 25)\n",
    "    else:\n",
    "        dataset = dataset.batch(config.batch_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_dataset(record_file, parser, config):\n",
    "    num_threads = tf.constant(config.num_threads, dtype=tf.int32)\n",
    "    dataset = tf.data.TFRecordDataset(record_file).map(\n",
    "        parser, num_parallel_calls=num_threads).repeat().batch(config.batch_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def convert_tokens(eval_file, qa_id, pp1, pp2):\n",
    "    answer_dict = {}\n",
    "    remapped_dict = {}\n",
    "    for qid, p1, p2 in zip(qa_id, pp1, pp2):\n",
    "        context = eval_file[str(qid)][\"context\"]\n",
    "        spans = eval_file[str(qid)][\"spans\"]\n",
    "        uuid = eval_file[str(qid)][\"uuid\"]\n",
    "        start_idx = spans[p1][0]\n",
    "        end_idx = spans[p2][1]\n",
    "        answer_dict[str(qid)] = context[start_idx: end_idx]\n",
    "        remapped_dict[uuid] = context[start_idx: end_idx]\n",
    "    return answer_dict, remapped_dict\n",
    "\n",
    "\n",
    "def evaluate(eval_file, answer_dict):\n",
    "    f1 = exact_match = total = 0\n",
    "    for key, value in answer_dict.items():\n",
    "        total += 1\n",
    "        ground_truths = eval_file[key][\"answers\"]\n",
    "        prediction = value\n",
    "        exact_match += metric_max_over_ground_truths(\n",
    "            exact_match_score, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(f1_score,\n",
    "                                            prediction, ground_truths)\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "    return {'exact_match': exact_match, 'f1': f1}\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.py\n",
    "\n",
    "import tensorflow as tf\n",
    "#from func import cudnn_gru, native_gru, dot_attention, summ, dropout, ptr_net\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, config, batch, word_mat=None, char_mat=None, trainable=True, opt=True):\n",
    "        self.config = config\n",
    "        self.global_step = tf.get_variable('global_step', shape=[], dtype=tf.int32,\n",
    "                                           initializer=tf.constant_initializer(0), trainable=False)\n",
    "        \n",
    "        ###################################################################################\n",
    "        # 09/05/2018\n",
    "        # the bath will return a question category \n",
    "        ###################################################################################\n",
    "        #         self.c, self.q, self.ch, self.qh, self.y1, self.y2, self.qa_id = batch.get_next()\n",
    "        \n",
    "        self.c, self.q, self.ch, self.qh, self.y1, self.y2, self.qa_id, self.ques_cat = batch.get_next()\n",
    "        \n",
    "        self.is_train = tf.get_variable(\n",
    "            \"is_train\", shape=[], dtype=tf.bool, trainable=False)\n",
    "        self.word_mat = tf.get_variable(\"word_mat\", initializer=tf.constant(\n",
    "            word_mat, dtype=tf.float32), trainable=False)\n",
    "        self.char_mat = tf.get_variable(\n",
    "            \"char_mat\", initializer=tf.constant(char_mat, dtype=tf.float32))\n",
    "\n",
    "        self.c_mask = tf.cast(self.c, tf.bool)\n",
    "        self.q_mask = tf.cast(self.q, tf.bool)\n",
    "        self.c_len = tf.reduce_sum(tf.cast(self.c_mask, tf.int32), axis=1)\n",
    "        self.q_len = tf.reduce_sum(tf.cast(self.q_mask, tf.int32), axis=1)\n",
    "        \n",
    "        ###################################################################################\n",
    "        # 09/05/2018\n",
    "        # may be, it will be necessary to add the mask and len variables\n",
    "        ###################################################################################\n",
    "        self.ques_cat_mask= tf.cast(self.ques_cat, tf.bool)\n",
    "        self.ques_cat_len = tf.reduce_sum(tf.cast(self.ques_cat_mask, tf.int32), axis=1)\n",
    "\n",
    "        if opt:\n",
    "            N, CL = config.batch_size, config.char_limit\n",
    "            self.c_maxlen = tf.reduce_max(self.c_len)\n",
    "            self.q_maxlen = tf.reduce_max(self.q_len)\n",
    "            self.c = tf.slice(self.c, [0, 0], [N, self.c_maxlen])\n",
    "            self.q = tf.slice(self.q, [0, 0], [N, self.q_maxlen])\n",
    "            self.c_mask = tf.slice(self.c_mask, [0, 0], [N, self.c_maxlen])\n",
    "            self.q_mask = tf.slice(self.q_mask, [0, 0], [N, self.q_maxlen])\n",
    "            self.ch = tf.slice(self.ch, [0, 0, 0], [N, self.c_maxlen, CL])\n",
    "            self.qh = tf.slice(self.qh, [0, 0, 0], [N, self.q_maxlen, CL])\n",
    "            self.y1 = tf.slice(self.y1, [0, 0], [N, self.c_maxlen])\n",
    "            self.y2 = tf.slice(self.y2, [0, 0], [N, self.c_maxlen])\n",
    "            \n",
    "            ###################################################################################\n",
    "            # 09/05/2018\n",
    "            # the slice of the question category\n",
    "            ###################################################################################\n",
    "            self.ques_cat_maxlen = tf.reduce_max(self.ques_cat_len)\n",
    "            self.ques_cat = tf.slice(self.ques_cat, [0,0], [N, self.ques_cat_maxlen])\n",
    "            \n",
    "        else:\n",
    "            self.c_maxlen, self.q_maxlen = config.para_limit, config.ques_limit\n",
    "\n",
    "        self.ch_len = tf.reshape(tf.reduce_sum(\n",
    "            tf.cast(tf.cast(self.ch, tf.bool), tf.int32), axis=2), [-1])\n",
    "        self.qh_len = tf.reshape(tf.reduce_sum(\n",
    "            tf.cast(tf.cast(self.qh, tf.bool), tf.int32), axis=2), [-1])\n",
    "\n",
    "        self.ready()\n",
    "\n",
    "        if trainable:\n",
    "            self.lr = tf.get_variable(\n",
    "                \"lr\", shape=[], dtype=tf.float32, trainable=False)\n",
    "            self.opt = tf.train.AdadeltaOptimizer(\n",
    "                learning_rate=self.lr, epsilon=1e-6)\n",
    "            grads = self.opt.compute_gradients(self.loss)\n",
    "            gradients, variables = zip(*grads)\n",
    "            capped_grads, _ = tf.clip_by_global_norm(\n",
    "                gradients, config.grad_clip)\n",
    "            self.train_op = self.opt.apply_gradients(\n",
    "                zip(capped_grads, variables), global_step=self.global_step)\n",
    "\n",
    "    def ready(self):\n",
    "        config = self.config\n",
    "        N, PL, QL, CL, d, dc, dg = config.batch_size, self.c_maxlen, self.q_maxlen, config.char_limit, config.hidden, config.char_dim, config.char_hidden\n",
    "        gru = cudnn_gru if config.use_cudnn else native_gru\n",
    "\n",
    "        with tf.variable_scope(\"emb\"):\n",
    "            with tf.variable_scope(\"char\"):\n",
    "                ch_emb = tf.reshape(tf.nn.embedding_lookup(\n",
    "                    self.char_mat, self.ch), [N * PL, CL, dc])\n",
    "                qh_emb = tf.reshape(tf.nn.embedding_lookup(\n",
    "                    self.char_mat, self.qh), [N * QL, CL, dc])\n",
    "                ch_emb = dropout(\n",
    "                    ch_emb, keep_prob=config.keep_prob, is_train=self.is_train)\n",
    "                qh_emb = dropout(\n",
    "                    qh_emb, keep_prob=config.keep_prob, is_train=self.is_train)\n",
    "                cell_fw = tf.contrib.rnn.GRUCell(dg)\n",
    "                cell_bw = tf.contrib.rnn.GRUCell(dg)\n",
    "                _, (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    cell_fw, cell_bw, ch_emb, self.ch_len, dtype=tf.float32)\n",
    "                ch_emb = tf.concat([state_fw, state_bw], axis=1)\n",
    "                _, (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    cell_fw, cell_bw, qh_emb, self.qh_len, dtype=tf.float32)\n",
    "                qh_emb = tf.concat([state_fw, state_bw], axis=1)\n",
    "                qh_emb = tf.reshape(qh_emb, [N, QL, 2 * dg])\n",
    "                ch_emb = tf.reshape(ch_emb, [N, PL, 2 * dg])\n",
    "\n",
    "            with tf.name_scope(\"word\"):\n",
    "                c_emb = tf.nn.embedding_lookup(self.word_mat, self.c)\n",
    "                q_emb = tf.nn.embedding_lookup(self.word_mat, self.q)\n",
    "                \n",
    "                ###################################################################\n",
    "                # 09/05/2018\n",
    "                # lookup the question category embedding\n",
    "                ################################################################\n",
    "                ques_cat_emb = tf.nn.embedding_lookup(self.word_mat, self.ques_cat)\n",
    "            \n",
    "            c_emb = tf.concat([c_emb, ch_emb], axis=2)\n",
    "\n",
    "            ###################################################################################\n",
    "            # 09/05/2018\n",
    "            # Add the question category in the concatenation of the question\n",
    "            ###################################################################################\n",
    "#             q_emb = tf.concat([q_emb, qh_emb], axis=2)\n",
    "            q_emb = tf.concat([q_emb, qh_emb, ques_cat_emb], axis=2)\n",
    "\n",
    "        with tf.variable_scope(\"encoding\"):\n",
    "            rnn = gru(num_layers=3, num_units=d, batch_size=N, input_size=c_emb.get_shape(\n",
    "            ).as_list()[-1], keep_prob=config.keep_prob, is_train=self.is_train)\n",
    "            c = rnn(c_emb, seq_len=self.c_len)\n",
    "            q = rnn(q_emb, seq_len=self.q_len)\n",
    "\n",
    "        with tf.variable_scope(\"attention\"):\n",
    "            qc_att = dot_attention(c, q, mask=self.q_mask, hidden=d,\n",
    "                                   keep_prob=config.keep_prob, is_train=self.is_train)\n",
    "            rnn = gru(num_layers=1, num_units=d, batch_size=N, input_size=qc_att.get_shape(\n",
    "            ).as_list()[-1], keep_prob=config.keep_prob, is_train=self.is_train)\n",
    "            att = rnn(qc_att, seq_len=self.c_len)\n",
    "\n",
    "        with tf.variable_scope(\"match\"):\n",
    "            self_att = dot_attention(\n",
    "                att, att, mask=self.c_mask, hidden=d, keep_prob=config.keep_prob, is_train=self.is_train)\n",
    "            rnn = gru(num_layers=1, num_units=d, batch_size=N, input_size=self_att.get_shape(\n",
    "            ).as_list()[-1], keep_prob=config.keep_prob, is_train=self.is_train)\n",
    "            match = rnn(self_att, seq_len=self.c_len)\n",
    "\n",
    "        with tf.variable_scope(\"pointer\"):\n",
    "            init = summ(q[:, :, -2 * d:], d, mask=self.q_mask,\n",
    "                        keep_prob=config.ptr_keep_prob, is_train=self.is_train)\n",
    "            pointer = ptr_net(batch=N, hidden=init.get_shape().as_list(\n",
    "            )[-1], keep_prob=config.ptr_keep_prob, is_train=self.is_train)\n",
    "            logits1, logits2 = pointer(init, match, d, self.c_mask)\n",
    "\n",
    "        with tf.variable_scope(\"predict\"):\n",
    "            outer = tf.matmul(tf.expand_dims(tf.nn.softmax(logits1), axis=2),\n",
    "                              tf.expand_dims(tf.nn.softmax(logits2), axis=1))\n",
    "            outer = tf.matrix_band_part(outer, 0, 15)\n",
    "            self.yp1 = tf.argmax(tf.reduce_max(outer, axis=2), axis=1)\n",
    "            self.yp2 = tf.argmax(tf.reduce_max(outer, axis=1), axis=1)\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=logits1, labels=tf.stop_gradient(self.y1))\n",
    "            losses2 = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=logits2, labels=tf.stop_gradient(self.y2))\n",
    "            self.loss = tf.reduce_mean(losses + losses2)\n",
    "\n",
    "    def get_loss(self):\n",
    "        return self.loss\n",
    "\n",
    "    def get_global_step(self):\n",
    "        return self.global_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import ujson as json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# from model import Model\n",
    "# from util import get_record_parser, convert_tokens, evaluate, get_batch_dataset, get_dataset\n",
    "\n",
    "\n",
    "def train(config):\n",
    "    with open(config.word_emb_file, \"r\") as fh:\n",
    "        word_mat = np.array(json.load(fh), dtype=np.float32)\n",
    "    with open(config.char_emb_file, \"r\") as fh:\n",
    "        char_mat = np.array(json.load(fh), dtype=np.float32)\n",
    "    with open(config.train_eval_file, \"r\") as fh:\n",
    "        train_eval_file = json.load(fh)\n",
    "    with open(config.dev_eval_file, \"r\") as fh:\n",
    "        dev_eval_file = json.load(fh)\n",
    "    with open(config.dev_meta, \"r\") as fh:\n",
    "        meta = json.load(fh)\n",
    "\n",
    "    dev_total = meta[\"total\"]\n",
    "\n",
    "    print(\"Building model...\")\n",
    "    parser = get_record_parser(config)\n",
    "    train_dataset = get_batch_dataset(config.train_record_file, parser, config)\n",
    "    dev_dataset = get_dataset(config.dev_record_file, parser, config)\n",
    "    handle = tf.placeholder(tf.string, shape=[])\n",
    "    iterator = tf.data.Iterator.from_string_handle(\n",
    "        handle, train_dataset.output_types, train_dataset.output_shapes)\n",
    "    train_iterator = train_dataset.make_one_shot_iterator()\n",
    "    dev_iterator = dev_dataset.make_one_shot_iterator()\n",
    "\n",
    "    model = Model(config, iterator, word_mat, char_mat)\n",
    "\n",
    "    sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    sess_config.gpu_options.allow_growth = True\n",
    "\n",
    "    loss_save = 100.0\n",
    "    patience = 0\n",
    "    lr = config.init_lr\n",
    "\n",
    "    with tf.Session(config=sess_config) as sess:\n",
    "        writer = tf.summary.FileWriter(config.log_dir)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        train_handle = sess.run(train_iterator.string_handle())\n",
    "        dev_handle = sess.run(dev_iterator.string_handle())\n",
    "        sess.run(tf.assign(model.is_train, tf.constant(True, dtype=tf.bool)))\n",
    "        sess.run(tf.assign(model.lr, tf.constant(lr, dtype=tf.float32)))\n",
    "\n",
    "        for _ in tqdm(range(1, config.num_steps + 1)):\n",
    "            global_step = sess.run(model.global_step) + 1\n",
    "            loss, train_op = sess.run([model.loss, model.train_op], feed_dict={\n",
    "                                      handle: train_handle})\n",
    "            if global_step % config.period == 0:\n",
    "                loss_sum = tf.Summary(value=[tf.Summary.Value(\n",
    "                    tag=\"model/loss\", simple_value=loss), ])\n",
    "                writer.add_summary(loss_sum, global_step)\n",
    "            if global_step % config.checkpoint == 0:\n",
    "                sess.run(tf.assign(model.is_train,\n",
    "                                   tf.constant(False, dtype=tf.bool)))\n",
    "                _, summ = evaluate_batch(\n",
    "                    model, config.val_num_batches, train_eval_file, sess, \"train\", handle, train_handle)\n",
    "                for s in summ:\n",
    "                    writer.add_summary(s, global_step)\n",
    "\n",
    "                metrics, summ = evaluate_batch(\n",
    "                    model, dev_total // config.batch_size + 1, dev_eval_file, sess, \"dev\", handle, dev_handle)\n",
    "                sess.run(tf.assign(model.is_train,\n",
    "                                   tf.constant(True, dtype=tf.bool)))\n",
    "\n",
    "                dev_loss = metrics[\"loss\"]\n",
    "                if dev_loss < loss_save:\n",
    "                    loss_save = dev_loss\n",
    "                    patience = 0\n",
    "                else:\n",
    "                    patience += 1\n",
    "                if patience >= config.patience:\n",
    "                    lr /= 2.0\n",
    "                    loss_save = dev_loss\n",
    "                    patience = 0\n",
    "                sess.run(tf.assign(model.lr, tf.constant(lr, dtype=tf.float32)))\n",
    "                for s in summ:\n",
    "                    writer.add_summary(s, global_step)\n",
    "                writer.flush()\n",
    "                filename = os.path.join(\n",
    "                    config.save_dir, \"model_{}.ckpt\".format(global_step))\n",
    "                saver.save(sess, filename)\n",
    "\n",
    "\n",
    "def evaluate_batch(model, num_batches, eval_file, sess, data_type, handle, str_handle):\n",
    "    answer_dict = {}\n",
    "    losses = []\n",
    "    for _ in tqdm(range(1, num_batches + 1)):\n",
    "        qa_id, loss, yp1, yp2, = sess.run(\n",
    "            [model.qa_id, model.loss, model.yp1, model.yp2], feed_dict={handle: str_handle})\n",
    "        answer_dict_, _ = convert_tokens(\n",
    "            eval_file, qa_id.tolist(), yp1.tolist(), yp2.tolist())\n",
    "        answer_dict.update(answer_dict_)\n",
    "        losses.append(loss)\n",
    "    loss = np.mean(losses)\n",
    "    metrics = evaluate(eval_file, answer_dict)\n",
    "    metrics[\"loss\"] = loss\n",
    "    loss_sum = tf.Summary(value=[tf.Summary.Value(\n",
    "        tag=\"{}/loss\".format(data_type), simple_value=metrics[\"loss\"]), ])\n",
    "    f1_sum = tf.Summary(value=[tf.Summary.Value(\n",
    "        tag=\"{}/f1\".format(data_type), simple_value=metrics[\"f1\"]), ])\n",
    "    em_sum = tf.Summary(value=[tf.Summary.Value(\n",
    "        tag=\"{}/em\".format(data_type), simple_value=metrics[\"exact_match\"]), ])\n",
    "    return metrics, [loss_sum, f1_sum, em_sum]\n",
    "\n",
    "\n",
    "def test(config):\n",
    "    with open(config.word_emb_file, \"r\") as fh:\n",
    "        word_mat = np.array(json.load(fh), dtype=np.float32)\n",
    "    with open(config.char_emb_file, \"r\") as fh:\n",
    "        char_mat = np.array(json.load(fh), dtype=np.float32)\n",
    "    with open(config.test_eval_file, \"r\") as fh:\n",
    "        eval_file = json.load(fh)\n",
    "    with open(config.test_meta, \"r\") as fh:\n",
    "        meta = json.load(fh)\n",
    "\n",
    "    total = meta[\"total\"]\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    test_batch = get_dataset(config.test_record_file, get_record_parser(\n",
    "        config, is_test=True), config).make_one_shot_iterator()\n",
    "\n",
    "    model = Model(config, test_batch, word_mat, char_mat, trainable=False)\n",
    "\n",
    "    sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    sess_config.gpu_options.allow_growth = True\n",
    "\n",
    "    with tf.Session(config=sess_config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(config.save_dir))\n",
    "        sess.run(tf.assign(model.is_train, tf.constant(False, dtype=tf.bool)))\n",
    "        losses = []\n",
    "        answer_dict = {}\n",
    "        remapped_dict = {}\n",
    "        for step in tqdm(range(total // config.batch_size + 1)):\n",
    "            qa_id, loss, yp1, yp2 = sess.run(\n",
    "                [model.qa_id, model.loss, model.yp1, model.yp2])\n",
    "            answer_dict_, remapped_dict_ = convert_tokens(\n",
    "                eval_file, qa_id.tolist(), yp1.tolist(), yp2.tolist())\n",
    "            answer_dict.update(answer_dict_)\n",
    "            remapped_dict.update(remapped_dict_)\n",
    "            losses.append(loss)\n",
    "        loss = np.mean(losses)\n",
    "        metrics = evaluate(eval_file, answer_dict)\n",
    "        with open(config.answer_file, \"w\") as fh:\n",
    "            json.dump(remapped_dict, fh)\n",
    "        print(\"Exact Match: {}, F1: {}\".format(\n",
    "            metrics['exact_match'], metrics['f1']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config.py\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from prepro import prepro\n",
    "from main import train, test\n",
    "\n",
    "flags = tf.flags\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "#Modification du chemin des sources de données\n",
    "#home = os.path.expanduser(\"~\")\n",
    "home = os.path.join(\"/home/ubuntu/spacework/R-NET-new-1\")\n",
    "#fin de la modification\n",
    "\n",
    "train_file = os.path.join(home, \"data\", \"squad\", \"train-v1.1.json\")\n",
    "dev_file = os.path.join(home, \"data\", \"squad\", \"dev-v1.1.json\")\n",
    "test_file = os.path.join(home, \"data\", \"squad\", \"dev-v1.1.json\")\n",
    "glove_word_file = os.path.join(home, \"data\", \"glove\", \"glove.840B.300d.txt\")\n",
    "\n",
    "################################################################################# \n",
    "# 27/04/2018  \n",
    "# List of the questions's categories\n",
    "train_ques_categories_file = os.path.join(home, \"data\", \"questions\", \"train_ques_categories.txt\")\n",
    "dev_ques_categories_file = os.path.join(home, \"data\", \"questions\", \"dev_ques_categories.txt\")\n",
    "test_ques_categories_file = os.path.join(home, \"data\", \"questions\", \"dev_ques_categories.txt\")\n",
    "\n",
    "target_dir = \"data\"\n",
    "log_dir = \"log/event\"\n",
    "save_dir = \"log/model\"\n",
    "answer_dir = \"log/answer\"\n",
    "train_record_file = os.path.join(target_dir, \"train.tfrecords\")\n",
    "dev_record_file = os.path.join(target_dir, \"dev.tfrecords\")\n",
    "test_record_file = os.path.join(target_dir, \"test.tfrecords\")\n",
    "word_emb_file = os.path.join(target_dir, \"word_emb.json\")\n",
    "char_emb_file = os.path.join(target_dir, \"char_emb.json\")\n",
    "train_eval = os.path.join(target_dir, \"train_eval.json\")\n",
    "dev_eval = os.path.join(target_dir, \"dev_eval.json\")\n",
    "test_eval = os.path.join(target_dir, \"test_eval.json\")\n",
    "dev_meta = os.path.join(target_dir, \"dev_meta.json\")\n",
    "test_meta = os.path.join(target_dir, \"test_meta.json\")\n",
    "word2idx_file = os.path.join(target_dir, \"word2idx.json\")\n",
    "char2idx_file = os.path.join(target_dir, \"char2idx.json\")\n",
    "answer_file = os.path.join(answer_dir, \"answer.json\")\n",
    "\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "if not os.path.exists(answer_dir):\n",
    "    os.makedirs(answer_dir)\n",
    "\n",
    "########################################################\n",
    "# 15/03/2018\n",
    "# add the prepro flag in the set, only for the notebook\n",
    "#########################################################\n",
    "flags.DEFINE_string(\"mode\", \"train\", \"train/debug/test\")\n",
    "# flags.DEFINE_string(\"mode\", \"prepro\", \"prepro\")\n",
    "\n",
    "flags.DEFINE_string(\"target_dir\", target_dir, \"\")\n",
    "flags.DEFINE_string(\"log_dir\", log_dir, \"\")\n",
    "flags.DEFINE_string(\"save_dir\", save_dir, \"\")\n",
    "flags.DEFINE_string(\"train_file\", train_file, \"\")\n",
    "flags.DEFINE_string(\"dev_file\", dev_file, \"\")\n",
    "flags.DEFINE_string(\"test_file\", test_file, \"\")\n",
    "flags.DEFINE_string(\"glove_word_file\", glove_word_file, \"\")\n",
    "\n",
    "flags.DEFINE_string(\"train_record_file\", train_record_file, \"\")\n",
    "flags.DEFINE_string(\"dev_record_file\", dev_record_file, \"\")\n",
    "flags.DEFINE_string(\"test_record_file\", test_record_file, \"\")\n",
    "flags.DEFINE_string(\"word_emb_file\", word_emb_file, \"\")\n",
    "flags.DEFINE_string(\"char_emb_file\", char_emb_file, \"\")\n",
    "flags.DEFINE_string(\"train_eval_file\", train_eval, \"\")\n",
    "flags.DEFINE_string(\"dev_eval_file\", dev_eval, \"\")\n",
    "flags.DEFINE_string(\"test_eval_file\", test_eval, \"\")\n",
    "flags.DEFINE_string(\"dev_meta\", dev_meta, \"\")\n",
    "flags.DEFINE_string(\"test_meta\", test_meta, \"\")\n",
    "flags.DEFINE_string(\"word2idx_file\", word2idx_file, \"\")\n",
    "flags.DEFINE_string(\"char2idx_file\", char2idx_file, \"\")\n",
    "flags.DEFINE_string(\"answer_file\", answer_file, \"\")\n",
    "\n",
    "################################################################################# \n",
    "# 27/04/2018  \n",
    "# List of the questions's categories\n",
    "################################################################################# \n",
    "flags.DEFINE_string(\"train_ques_categories_file\", train_ques_categories_file, \"\")\n",
    "flags.DEFINE_string(\"dev_ques_categories_file\", dev_ques_categories_file, \"\")\n",
    "flags.DEFINE_string(\"test_ques_categories_file\", test_ques_categories_file, \"\")\n",
    "\n",
    "\n",
    "flags.DEFINE_integer(\"glove_char_size\", 94, \"Corpus size for Glove\")\n",
    "flags.DEFINE_integer(\"glove_word_size\", int(2.2e6), \"Corpus size for Glove\")\n",
    "flags.DEFINE_integer(\"glove_dim\", 300, \"Embedding dimension for Glove\")\n",
    "flags.DEFINE_integer(\"char_dim\", 8, \"Embedding dimension for char\")\n",
    "\n",
    "flags.DEFINE_integer(\"para_limit\", 400, \"Limit length for paragraph\")\n",
    "flags.DEFINE_integer(\"ques_limit\", 50, \"Limit length for question\")\n",
    "flags.DEFINE_integer(\"test_para_limit\", 1000,\n",
    "                     \"Max length for paragraph in test\")\n",
    "flags.DEFINE_integer(\"test_ques_limit\", 100, \"Max length of questions in test\")\n",
    "flags.DEFINE_integer(\"char_limit\", 16, \"Limit length for character\")\n",
    "flags.DEFINE_integer(\"word_count_limit\", -1, \"Min count for word\")\n",
    "flags.DEFINE_integer(\"char_count_limit\", -1, \"Min count for char\")\n",
    "\n",
    "flags.DEFINE_integer(\"capacity\", 15000, \"Batch size of dataset shuffle\")\n",
    "flags.DEFINE_integer(\"num_threads\", 4, \"Number of threads in input pipeline\")\n",
    "\n",
    "flags.DEFINE_boolean(\"use_cudnn\", True, \"Whether to use cudnn (only for GPU)\")\n",
    "#flags.DEFINE_boolean(\"use_cudnn\", False, \"Whether to use cudnn (only for GPU)\")\n",
    "flags.DEFINE_boolean(\"is_bucket\", False, \"Whether to use bucketing\")\n",
    "flags.DEFINE_list(\"bucket_range\", [40, 361, 40], \"range of bucket\")\n",
    "\n",
    "#Modification de la taille du batch\n",
    "#flags.DEFINE_integer(\"batch_size\", 64, \"Batch size\")\n",
    "flags.DEFINE_integer(\"batch_size\", 48, \"Batch size\")\n",
    "flags.DEFINE_integer(\"num_steps\", 60000, \"Number of steps\")\n",
    "flags.DEFINE_integer(\"checkpoint\", 1000, \"checkpoint for evaluation\")\n",
    "flags.DEFINE_integer(\"period\", 100, \"period to save batch loss\")\n",
    "flags.DEFINE_integer(\"val_num_batches\", 150, \"Num of batches for evaluation\")\n",
    "flags.DEFINE_float(\"init_lr\", 0.5, \"Initial lr for Adadelta\")\n",
    "flags.DEFINE_float(\"keep_prob\", 0.7, \"Keep prob in rnn\")\n",
    "flags.DEFINE_float(\"ptr_keep_prob\", 0.7, \"Keep prob for pointer network\")\n",
    "flags.DEFINE_float(\"grad_clip\", 5.0, \"Global Norm gradient clipping rate\")\n",
    "flags.DEFINE_integer(\"hidden\", 75, \"Hidden size\")\n",
    "flags.DEFINE_integer(\"char_hidden\", 100, \"GRU dim for char\")\n",
    "flags.DEFINE_integer(\"patience\", 3, \"Patience for lr decay\")\n",
    "\n",
    "################################################################################# \n",
    "# 02/05/2018  \n",
    "# limits for the question category\n",
    "################################################################################# \n",
    "flags.DEFINE_integer(\"ques_category_limit\", 1, \"Limit length for question category\")\n",
    "flags.DEFINE_integer(\"ques_category_char_limit\", 6, \"Limit length for question category character\")\n",
    "\n",
    "# Extensions (Uncomment corresponding line in download.sh to download the required data)\n",
    "glove_char_file = os.path.join(\n",
    "    home, \"data\", \"glove\", \"glove.840B.300d-char.txt\")\n",
    "flags.DEFINE_string(\"glove_char_file\", glove_char_file,\n",
    "                    \"Glove character embedding\")\n",
    "flags.DEFINE_boolean(\"pretrained_char\", False,\n",
    "                     \"Whether to use pretrained char embedding\")\n",
    "\n",
    "fasttext_file = os.path.join(home, \"data\", \"fasttext\", \"wiki-news-300d-1M.vec\")\n",
    "flags.DEFINE_string(\"fasttext_file\", fasttext_file, \"Fasttext word embedding\")\n",
    "#flags.DEFINE_boolean(\"fasttext\", False, \"Whether to use fasttext\")\n",
    "flags.DEFINE_boolean(\"fasttext\", True, \"Whether to use fasttext\")\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    config = flags.FLAGS\n",
    "    if config.mode == \"train\":\n",
    "        print(\"train\")\n",
    "        train(config)\n",
    "    elif config.mode == \"prepro\":\n",
    "        print(\"prepro\")\n",
    "        prepro(config)\n",
    "        print('end of prepro')\n",
    "    elif config.mode == \"debug\":\n",
    "        print(\"debug\")\n",
    "        config.num_steps = 2\n",
    "        config.val_num_batches = 1\n",
    "        config.checkpoint = 1\n",
    "        config.period = 1\n",
    "        train(config)\n",
    "    elif config.mode == \"test\":\n",
    "        print(\"test\")\n",
    "        test(config)\n",
    "    else:\n",
    "        print(\"Unknown mode\")\n",
    "        #exit(0)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Building model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4ddc899a731b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mrun_w_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margument\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# if __name__ == \"__main__\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4ddc899a731b>\u001b[0m in \u001b[0;36mrun_w_exit\u001b[0;34m(main, argument)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknown_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#fin de l'ajout de la methode\\n\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-715852871377>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"prepro\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prepro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spacework/R-NET-new-1/main.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtrain_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_1.5.0-rc1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_1.5.0-rc1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_1.5.0-rc1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_1.5.0-rc1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_1.5.0-rc1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from tensorflow.python.platform import flags\n",
    "import argparse\n",
    "from tensorflow.python.platform.flags import FLAGS\n",
    "\n",
    "def run_w_exit(main=None, argument=None):\n",
    "    arguments = [a for a in sys.argv]\n",
    "    flag = FLAGS(sys.argv, known_only=True)\n",
    "    main(argument)\n",
    "#fin de l'ajout de la methode\\n\",\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_w_exit(main=main, argument=\"train\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     run_w_exit(main=main, argument=\" prepro\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
